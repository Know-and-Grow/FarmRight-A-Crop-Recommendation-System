# -*- coding: utf-8 -*-
"""Model_Training_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18NmTt7mSbGioiiDtqzaFDOM2AJ5qTJxM
"""

import warnings
warnings.simplefilter(action='ignore', category=UserWarning)

"""### Cloning the Repository"""

# Get the repository containing Master dataset
!git clone https://github.com/Know-and-Grow/FarmRight.git

# !rm -rf <folder_name> | to delete any folder(empty / non empty both)
#!rm -rf Know-and-Grow/

# Importing Python Libraries
import csv
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns;
import numpy as np
# ------------------------------------------------------------------------------
# RUN THIS & DIRECTLY SKIP TO "Splitting the Dataset into Training and Testing"
# ------------------------------------------------------------------------------
# final_df = pd.read_csv('/content/Know-and-Grow/Datasets/MasterDB_final.csv')

"""### Master(Combined) df to Final(Pre-processed) df processing"""

master_df = pd.read_csv('/content/FarmRight/Datasets/MasterDB.csv')
master_df['Season'] = master_df['Season'].str.upper()
master_df['Crop'] = master_df['Crop'].str.upper()
# Dropping first column of dataframe
#master_df = master_df.iloc[: , 1:]
master_df=master_df.drop(columns=['Prod/Area'])
master_df.head()

master_df.shape

cols=['State_Name','District_Name','Season','N','OC','P','K','ACIDIC','NEUTRAL','BASIC','Temperature','Rainfall','Crop']
master_df[cols]

"""### Preparing the Dataset"""

from sklearn.preprocessing import OneHotEncoder,LabelEncoder

le_district = LabelEncoder()
le_season = LabelEncoder()
le_crop=LabelEncoder()
le_state=LabelEncoder()
master_df['District_Name']=le_district.fit_transform(master_df['District_Name'])
district_mapping = dict(zip(le_district.classes_, le_district.transform(le_district.classes_)))
master_df['Season']=le_season.fit_transform(master_df['Season'])
season_mapping = dict(zip(le_season.classes_, le_season.transform(le_season.classes_)))
master_df['Crop']=le_crop.fit_transform(master_df['Crop'])
crop_mapping = dict(zip(le_crop.classes_, le_crop.transform(le_crop.classes_)))
master_df['State_Name']=le_state.fit_transform(master_df['State_Name'])
state_mapping = dict(zip(le_state.classes_, le_state.transform(le_state.classes_)))

master_df = pd.get_dummies(master_df, columns=["Crop"], prefix = ["Crop"])

# Wrap output text
from IPython.display import HTML, display

def set_css():
  display(HTML('''
  <style>
    pre {
        white-space: pre-wrap;
    }
  </style>
  '''))
get_ipython().events.register('pre_run_cell', set_css)

print(crop_mapping)

print(district_mapping)

print(state_mapping)

print(season_mapping)

master_df.head()

#list(district_mapping.keys())[list(district_mapping.values()).index(34)]

g1=master_df.groupby(['State_Name','District_Name','Season','Crop_Year'])['Crop_0'].sum().reset_index()

for i in range(1,124):
  g=master_df.groupby(['State_Name','District_Name','Season','Crop_Year'])['Crop_'+str(i)].sum().reset_index()
  g1=pd.merge(g1,g,how='inner',on=['State_Name','District_Name','Season','Crop_Year'])

g1.head()

new = master_df.filter(['State_Name','District_Name','Season','Crop_Year','N','OC','P','K','ACIDIC','NEUTRAL','BASIC','Temperature','Rainfall'], axis=1)
final_df=pd.merge(g1,new,how='left',on=['State_Name','District_Name','Season','Crop_Year'])
final_df=final_df.drop_duplicates()
final_df.head()

# final_df.to_csv('MasterDB_final.csv')

"""### Splitting the Dataset into Training and Testing"""

X=final_df[['State_Name','District_Name','Season','N','OC','P','K','ACIDIC','NEUTRAL','BASIC','Temperature','Rainfall']]
X.head()

y=final_df.drop(columns=['State_Name','District_Name','Season','Crop_Year','N','OC','P','K','ACIDIC','NEUTRAL','BASIC','Temperature','Rainfall'])
y.head()

# Converting temp from Celsius to Kelvin
X['Temperature'] = X['Temperature'].apply(lambda x: x+273)
#X.loc[X['Temperature']<0]

# Splitting the model in training & testing datasets
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42)

print(X_train.shape)
print(X_test.shape)

"""### Building and Training Model"""

!pip install scikit-multilearn
# Importing Python models
from skmultilearn.problem_transform import BinaryRelevance
from skmultilearn.problem_transform import ClassifierChain
from skmultilearn.problem_transform import LabelPowerset
from sklearn.multioutput import MultiOutputClassifier

from sklearn.naive_bayes import GaussianNB,MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score,hamming_loss,precision_score,recall_score,f1_score

# Build the model
def build_model(model,mlb_estimator,xtrain,ytrain,xtest,ytest):
    clf = mlb_estimator(model)
    clf.fit(xtrain,ytrain)
    clf_predictions = clf.predict(xtest)
    acc = accuracy_score(ytest,clf_predictions)
    ham = hamming_loss(ytest,clf_predictions)
    prec = precision_score(ytest,clf_predictions,average="samples")
    rec = recall_score(ytest,clf_predictions,average="samples")
    f1 = f1_score(ytest,clf_predictions,average="samples")
    result = {"hamming_loss":ham,"precision":prec,"recall":rec,"f1_score":f1}
    print(str(mlb_estimator)+" "+str(model))
    print(result)
    return clf

# Building RELEVANT MODELS to get Optimal Results
# build_model(MultinomialNB(),BinaryRelevance,X_train,y_train,X_test,y_test)

# build_model(MultinomialNB(),ClassifierChain,X_train,y_train,X_test,y_test)

# build_model(GaussianNB(),BinaryRelevance,X_train,y_train,X_test,y_test)

# build_model(GaussianNB(),ClassifierChain,X_train,y_train,X_test,y_test)

# build_model(DecisionTreeClassifier(),BinaryRelevance,X_train,y_train,X_test,y_test)

# build_model(DecisionTreeClassifier(),ClassifierChain,X_train,y_train,X_test,y_test)

# build_model(RandomForestClassifier(),ClassifierChain,X_train,y_train,X_test,y_test)

#https://www.analyticsvidhya.com/blog/2020/03/beginners-guide-random-forest-hyperparameter-tuning/
#https://medium.com/@saugata.paul1010/a-detailed-case-study-on-multi-label-classification-with-machine-learning-algorithms-and-72031742c9aa

# temp=model.predict(np.array(X_test.iloc[1]).reshape(1,-1))

# print(temp.toarray)

# np.array(X_test.iloc[1]).reshape(1,-1)

"""### *Optimization: Lesser Hamming Loss & Better Precision, Recall & F1 Score to be achieved*"""

model=build_model(RandomForestClassifier(random_state=42,max_depth=20,min_samples_split=14,n_estimators=300),BinaryRelevance,X_train,y_train,X_test,y_test)

# Compressing the model
import joblib
joblib.dump(model,'model_jlib',compress=7)

import os 
os.path.getsize("/content/model_jlib")

# save the model to disk
# loading library
import pickle
# create an iterator object with write permission - model.pkl
with open('model.pkl', 'wb') as files:
    pickle.dump(model, files)

# for i in np.arange(50,1000,50):
#   model=build_model(RandomForestClassifier(random_state=42,max_depth=20,min_samples_split=14,n_estimators=i),BinaryRelevance,X_train,y_train,X_test,y_test)

# from sklearn.model_selection import GridSearchCV

# # HYPERPARAMETERS
# # Number of trees in random forest
# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 15)]
# # Number of features to consider at every split
# max_features = ['auto', 'sqrt']
# # Maximum number of levels in tree
# max_depth = [int(x) for x in np.linspace(10, 110, num = 12)]
# max_depth.append(None)
# # Minimum number of samples required to split a node
# min_samples_split = [20, 50, 100]
# # Minimum number of samples required at each leaf node
# min_samples_leaf = [100, 200, 400]
# # Method of selecting samples for training each tree
# bootstrap = [True, False]
# # Create the random grid
# params = {'classifier__n_estimators': n_estimators,
#                'classifier__max_features': max_features,
#                'classifier__max_depth': max_depth,
#                'classifier__min_samples_split': min_samples_split,
#                'classifier__min_samples_leaf': min_samples_leaf,
#                'classifier__bootstrap': bootstrap}

# base_model=BinaryRelevance(RandomForestClassifier(random_state=42))
# rsearch_cv = GridSearchCV(estimator=base_model, param_grid=params, cv=3, scoring='f1_samples', n_jobs=-1, verbose=0,return_train_score=True)
# rsearch_cv.fit(X_train, y_train)

# best_random = rsearch_cv.best_estimator_
# predictions=best_random.predict(X_test)
# acc = accuracy_score(y_test,predictions)
# ham = hamming_loss(y_test,predictions)
# prec = precision_score(y_test,predictions,average="samples")
# rec = recall_score(y_test,predictions,average="samples")
# f1 = f1_score(y_test,predictions,average="samples")
# result = {"hamming_loss":ham,"precision":prec,"recall":rec,"f1_score":f1}

# print(rsearch_cv.best_estimator_)

# print(result)

"""### Making Single Prediction"""

state=input('Enter State ')
state=state.upper()
state_code=state_mapping[state]

district=input('Enter District ')
district=district.upper()
district_code=district_mapping[district]

season=input('Enter Season ')
season=season.upper()
season_code=season_mapping[season]

soil_df=pd.read_csv('/content/Know-and-Grow/Datasets/Soil_DB.csv')
soil_df = soil_df.iloc[: , 2:]
soil=soil_df.loc[(soil_df['State']==state) & (soil_df['District']==district)]
soil['N'] = soil['N'].astype(str).str[:-1].astype(float, errors = 'raise')
soil['OC'] = soil['OC'].astype(str).str[:-1].astype(float, errors = 'raise')
soil['P'] = soil['P'].astype(str).str[:-1].astype(float, errors = 'raise')
soil['K'] = soil['K'].astype(str).str[:-1].astype(float, errors = 'raise')
soil['ACIDIC']=soil[['AS%','SrAc%','HAc%','MAc%','SlAc%']].sum(axis=1)
soil['NEUTRAL']=soil['N%']
soil['BASIC']=soil[['MAl%','SlAl%']].sum(axis=1)
soil=soil.drop(columns=['AS%','SrAc%','HAc%','MAc%','SlAc%','N%','MAl%','SlAl%','State','District'])
soil.head()

temp=float(input('Enter Temperature ')) #Kelvin
rain=float(input('Enter Rainfall '))

soil=np.array(soil).reshape(1,-1)

soil=np.append(soil,np.array([[temp,rain]]),axis=1)

soil=np.insert(soil,0,np.array((state_code)),1)
soil=np.insert(soil,1,np.array((district_code)),1)
soil=np.insert(soil,2,np.array((season_code)),1)

soil

temp=model.predict(soil)

temp=temp.todense() #to convert into simple array data type

temp=np.squeeze(np.asarray(temp))

crop_index=[]
for i in range(len(temp)):
  if(temp[i]==1):
    crop_index.append(i)

# Print list of recommended crops 
recommendation=[crop for crop, code in crop_mapping.items() if code in crop_index]
recommendation

master_df=pd.read_csv('/content/Know-and-Grow/Datasets/MasterDB.csv')
master_df = master_df.iloc[: , 1:]
master_df=master_df[(master_df['Crop'].isin(recommendation))]
df_current=master_df.loc[(master_df['State_Name']==state) & \
                         (master_df['District_Name']==district) \
                         & (master_df['Season']==season)]
df_current=df_current.drop(columns=['Crop_Year','N','OC','P','K','ACIDIC','NEUTRAL','BASIC','Temperature','Rainfall'])          
df_current.head(30)

avg_prod_area=df_current.groupby(['Crop'])['Prod/Area'].mean().reset_index()
avg_prod_area=avg_prod_area.sort_values(by=['Prod/Area'],ascending=False)
avg_prod_area.head()

avg_prod_area['Crop'].reset_index(drop=True)